{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from image_classification_simulation.data.office31_loader import Office31Loader\n",
    "from image_classification_simulation.models.clustering_tools import show_grid_images\n",
    "from image_classification_simulation.image_search import ImageSimilaritySearch\n",
    "from image_classification_simulation.utils.visualization_utils import show_grid_images\n",
    "from image_classification_simulation.models.clustering_tools import get_clustering_metrics\n",
    "from image_classification_simulation.models.clustering_tools import (\n",
    "    eval_clustering_performance, accuracy_per_class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"num_workers\": 2,\n",
    "    'batch_size': 32,\n",
    "    \"image_size\":224,\n",
    "    \"train_test_split\":-1,\n",
    "}\n",
    "office_loader = Office31Loader(\n",
    "    data_dir=\"../examples/data/domain_adaptation_images/amazon/images/\",\n",
    "    eval_dir=\"../examples/data/domain_adaptation_images/dslr/images/\",\n",
    "     hyper_params=hparams)\n",
    "# office_loader.setup('fit')\n",
    "# train_loader = office_loader.train_dataloader()\n",
    "# val_loader = office_loader.val_dataloader()\n",
    "# test_loader = office_loader.test_dataloader()\n",
    "# # /network/projects/aia/img_classif_sim/vit/output/best_model\n",
    "office_loader.setup('eval')\n",
    "eval_loader = office_loader.eval_dataloader(shuffle=False)\n",
    "office_loader.setup('infer')\n",
    "train_loader = office_loader.train_dataloader(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams_resnet = {\n",
    "    \"clustering_alg\": \"nn\",\n",
    "    \"num_neighbors\":20,\n",
    "    \"radius\":0.5,\n",
    "    \"n_jobs\":2,\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"batch_size\": 100,\n",
    "    \"pretrained\": True,\n",
    "    \"num_classes\": 31,\n",
    "    \"path_to_model\": \"../examples/resnet/output/best_model/model.ckpt\",\n",
    "    \"architecture\": \"resnet\",\n",
    "    \"num_clusters\": 31,\n",
    "    \"random_state\": 0,\n",
    "    \"clustering_batch_size\": 100,\n",
    "    \"size\": 256,\n",
    "    \"reassignment_ratio\": 0.05,\n",
    "    \"path_cluster_ids\": \"../debug/dataset_cluster_ids.csv\",\n",
    "}\n",
    "hparams_vit = {\n",
    "    # \"clustering_alg\": \"MiniBatchKMeans\",\n",
    "    \"clustering_alg\": \"nn\",\n",
    "    \"num_neighbors\":20,\n",
    "    \"radius\":0.5,\n",
    "    \"n_jobs\":2,\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"pretrained\": True,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_classes\": 31,\n",
    "    \"path_to_model\": \"/network/projects/aia/img_classif_sim/vit/output/best_model/model.ckpt\",\n",
    "    \"architecture\": \"vit\",\n",
    "    \"num_clusters\": 100,\n",
    "    \"random_state\": 0,\n",
    "    \"clustering_batch_size\": 1024,\n",
    "    \"reassignment_ratio\": 0.01,\n",
    "    \"init\":'random',\n",
    "    \"path_cluster_ids\": \"../debug/dataset_cluster_ids.csv\",\n",
    "}\n",
    "hparams_ae = {\n",
    "    \"clustering_alg\": \"MiniBatchKMeans\",\n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"pretrained\": True,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_classes\": 31,\n",
    "    \"path_to_model\": \"/network/projects/aia/img_classif_sim/conv_ae/output/best_model/model.ckpt\",\n",
    "    \"architecture\": \"conv_ae\",\n",
    "    \"num_clusters\": 32,\n",
    "    \"random_state\": 0,\n",
    "    \"clustering_batch_size\": 100,\n",
    "    \"reassignment_ratio\": 0.05,\n",
    "    \"path_cluster_ids\": \"../debug/dataset_cluster_ids.csv\",\n",
    "}\n",
    "hparams_cnn = {\n",
    "        \"clustering_alg\": \"nn\",\n",
    "        \"num_neighbors\":20,\n",
    "        \"radius\":0.5,\n",
    "        \"n_jobs\":2,\n",
    "        \"loss\": \"CrossEntropyLoss\",\n",
    "        \"batch_size\": 124,\n",
    "        \"num_channels\": 3,\n",
    "        \"pretrained\": True,\n",
    "        \"num_classes\": 31,\n",
    "        \"img_size\": 224,\n",
    "        \"path_to_model\": \"/network/projects/aia/img_classif_sim/classic_cnn/output/best_model/model.ckpt\",\n",
    "        \"architecture\": \"classic-cnn\",\n",
    "        \"num_clusters\": 31,\n",
    "        \"random_state\": 0,\n",
    "        \"clustering_batch_size\": 124,\n",
    "        \"reassignment_ratio\": 0.01,\n",
    "        \"path_cluster_ids\": \"../debug/dataset_cluster_ids.csv\",\n",
    "    }\n",
    "\n",
    "archs = {\n",
    "    \"resnet\": hparams_resnet,\n",
    "    \"vit\": hparams_vit,\n",
    "    # \"ae\": hparams_ae,\n",
    "    \"cnn\":hparams_cnn\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_true = [label for image, label in office_loader.eval_set]\n",
    "len(labels_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = image_search.predict(office_loader.eval_dataloader())\n",
    "len(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following metrics do not work when we are using nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rand_score': 0.024261940161551716,\n",
       " 'adjusted_rand_score': 0.024261940161551716,\n",
       " 'mutual_info_score': 0.5971366373315987}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_clustering_metrics(labels_true, labels_pred)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rand_score': 0.03455624029458484,\n",
       " 'adjusted_rand_score': 0.03455624029458484,\n",
       " 'mutual_info_score': 0.6542970012509423}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use either images from the evaluation or the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../examples/data/domain_adaptation_images/amazon/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '../examples/data/domain_adaptation_images/dslr/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for arch in archs:\n",
    "    image_search = ImageSimilaritySearch(archs[arch], office_loader)\n",
    "    image_search.setup()\n",
    "    for class_name in office_loader.dataset.class_to_idx:\n",
    "        print(class_name)\n",
    "        path = eval_dir+\"{}/frame_0001.jpg\".format(class_name)\n",
    "        query_res = image_search.find_similar_images(path,None)\n",
    "        fig,_ = show_grid_images(\n",
    "            query_res['image_path'].tolist(),\n",
    "            num_rows=5,\n",
    "            num_cols=5,\n",
    "            )\n",
    "        fig.savefig('./results/'+arch+'/'+class_name+'.png',format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search = ImageSimilaritySearch(archs['resnet'], office_loader)\n",
    "image_search.setup()\n",
    "class_ids = office_loader.labels\n",
    "true_labels = [label for img, label in office_loader.eval_set]\n",
    "total_score, res = eval_clustering_performance(\n",
    "        class_ids,\n",
    "        true_labels,\n",
    "        image_search.clustering.find_neighbors,\n",
    "        eval_loader,\n",
    "        5\n",
    "    )\n",
    "class_acc = accuracy_per_class(true_labels, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "back_pack 0.8\n",
      "bike 0.9238095238095239\n",
      "bike_helmet 0.85\n",
      "bookcase 0.6\n",
      "bottle 0.85\n",
      "calculator 0.8\n",
      "desk_chair 0.8923076923076924\n",
      "desk_lamp 0.4\n",
      "desktop_computer 0.14666666666666667\n",
      "file_cabinet 0.4533333333333333\n",
      "headphones 0.5230769230769231\n",
      "keyboard 0.52\n",
      "laptop_computer 0.44166666666666665\n",
      "letter_tray 0.5\n",
      "mobile_phone 0.16129032258064516\n",
      "monitor 0.35454545454545455\n",
      "mouse 0.9166666666666666\n",
      "mug 0.925\n",
      "paper_notebook 0.74\n",
      "pen 0.74\n",
      "phone 0.6923076923076923\n",
      "printer 0.4\n",
      "projector 0.30434782608695654\n",
      "punchers 0.2222222222222222\n",
      "ring_binder 0.26\n",
      "ruler 0.6571428571428571\n",
      "scissors 0.9222222222222223\n",
      "speaker 0.16153846153846155\n",
      "stapler 0.2761904761904762\n",
      "tape_dispenser 0.34545454545454546\n",
      "trash_can 0.4266666666666667\n"
     ]
    }
   ],
   "source": [
    "for key,value in office_loader.dataset.class_to_idx.items():\n",
    "    print(key,class_acc[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search = ImageSimilaritySearch(archs['vit'], office_loader)\n",
    "image_search.setup()\n",
    "class_ids = office_loader.labels\n",
    "true_labels = [label for img, label in office_loader.eval_set]\n",
    "total_score, res = eval_clustering_performance(\n",
    "        class_ids,\n",
    "        true_labels,\n",
    "        image_search.clustering.find_neighbors,\n",
    "        eval_loader,\n",
    "        5\n",
    "    )\n",
    "class_acc = accuracy_per_class(true_labels, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_acc = accuracy_per_class(true_labels, res)\n",
    "r = dict()\n",
    "for key,value in office_loader.dataset.class_to_idx.items():\n",
    "    r[key]=class_acc[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>back_pack</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike_helmet</th>\n",
       "      <td>0.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookcase</th>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottle</th>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculator</th>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desk_chair</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desk_lamp</th>\n",
       "      <td>0.842857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desktop_computer</th>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_cabinet</th>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headphones</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptop_computer</th>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>letter_tray</th>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile_phone</th>\n",
       "      <td>0.761290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monitor</th>\n",
       "      <td>0.327273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mug</th>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_notebook</th>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>printer</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projector</th>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punchers</th>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ring_binder</th>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruler</th>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scissors</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <td>0.438462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stapler</th>\n",
       "      <td>0.819048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tape_dispenser</th>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trash_can</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "back_pack         1.000000\n",
       "bike              1.000000\n",
       "bike_helmet       0.941667\n",
       "bookcase          0.800000\n",
       "bottle            0.125000\n",
       "calculator        0.933333\n",
       "desk_chair        1.000000\n",
       "desk_lamp         0.842857\n",
       "desktop_computer  0.813333\n",
       "file_cabinet      0.946667\n",
       "headphones        1.000000\n",
       "keyboard          0.600000\n",
       "laptop_computer   0.550000\n",
       "letter_tray       0.275000\n",
       "mobile_phone      0.761290\n",
       "monitor           0.327273\n",
       "mouse             1.000000\n",
       "mug               0.925000\n",
       "paper_notebook    0.480000\n",
       "pen               0.980000\n",
       "phone             1.000000\n",
       "printer           1.000000\n",
       "projector         0.913043\n",
       "punchers          0.433333\n",
       "ring_binder       0.660000\n",
       "ruler             0.600000\n",
       "scissors          1.000000\n",
       "speaker           0.438462\n",
       "stapler           0.819048\n",
       "tape_dispenser    0.727273\n",
       "trash_can         1.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=r.values(),index=r.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5290e27e2982e14914fc743eb271efd553283179009389fbf33907a815e7eb33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
