{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vh6Xra9NBBt-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "from torch import nn, optim\n",
        "from torch import tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "from torch.optim import lr_scheduler\n",
        "from image_classification_simulation.data.office31_loader import Office31Loader\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvErkR0diAWo",
        "outputId": "fe1d2e49-d2a0-4442-faa6-8254e547030c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image size set to: 200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "office_loader = Office31Loader(data_dir=\"../examples/data/amazon/images/\", hyper_params={\"num_workers\": 2, 'batch_size': 32})\n",
        "office_loader.setup('fit')\n",
        "train_loader = office_loader.train_dataloader()\n",
        "val_loader = office_loader.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhZlJdFZBluM",
        "outputId": "20237c51-80b8-4489-8181-33ada301137e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resnet(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear1): Linear(in_features=512, out_features=31, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def dfs_freeze(model):\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "def dfs_unfreeze(model):\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, feature_extractor: nn.Module):\n",
        "        super(Resnet, self).__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        layers = list(self.feature_extractor.children())[:-1]\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "        # dfs_freeze(self.feature_extractor)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = torch.nn.Linear(512, 31)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batch_images: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        # self.feature_extractor.eval()\n",
        "        # with torch.no_grad():\n",
        "        z_x = self.feature_extractor.forward(batch_images)\n",
        "\n",
        "        z_x = self.flatten(z_x)\n",
        "        logits = self.linear1(z_x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "model = None\n",
        "convolutional_network = resnet18(pretrained=True)\n",
        "model = Resnet(convolutional_network)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sJHBTV_pGlwk"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    test_loader):\n",
        "    \"\"\"\n",
        "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
        "    \"\"\"\n",
        "    preds, true  = [], []\n",
        "    correct, size = 0 , 0\n",
        "    for batch_images, batch_labels in test_loader:\n",
        "      logits = model(batch_images.to(device)).detach().data\n",
        "      probs = softmax(logits,1)\n",
        "      preds = torch.argmax(probs,1)\n",
        "      correct+= torch.sum( preds == batch_labels.to(device) ).item() \n",
        "      size+=batch_images.size(0)\n",
        "\n",
        "    return 100*correct / size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO_MRL0Oqu9u",
        "outputId": "25f6cd84-9217-4431-8e74-f78d5dfbdc78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "end of epoch 1 total loss is 3.4459990471601487 train accuracy is 4.889589905362776.\n",
            "Loss 3.2255935668945312 and validation accuracy 10.320284697508896: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 2 total loss is 3.147828111052513 train accuracy is 18.454258675078865.\n",
            "Loss 2.970505952835083 and validation accuracy 24.91103202846975: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 3 total loss is 2.917146733403206 train accuracy is 35.05520504731861.\n",
            "Loss 2.7609496116638184 and validation accuracy 39.50177935943061: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 4 total loss is 2.700843733549118 train accuracy is 47.55520504731861.\n",
            "Loss 2.5627989768981934 and validation accuracy 46.619217081850536: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 5 total loss is 2.4955227226018906 train accuracy is 55.678233438485805.\n",
            "Loss 2.3672666549682617 and validation accuracy 51.245551601423486: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 6 total loss is 2.3058450877666474 train accuracy is 60.72555205047318.\n",
            "Loss 2.183184862136841 and validation accuracy 55.51601423487544: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 7 total loss is 2.1345103427767755 train accuracy is 64.70820189274448.\n",
            "Loss 2.0100622177124023 and validation accuracy 59.07473309608541: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 8 total loss is 1.9819140434265137 train accuracy is 67.74447949526814.\n",
            "Loss 1.8484086990356445 and validation accuracy 62.63345195729537: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 9 total loss is 1.8466781795024871 train accuracy is 69.75552050473186.\n",
            "Loss 1.7059274911880493 and validation accuracy 64.05693950177935: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 10 total loss is 1.7270418494939803 train accuracy is 71.64826498422713.\n",
            "Loss 1.5768142938613892 and validation accuracy 65.83629893238434: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 11 total loss is 1.6208278134465217 train accuracy is 72.98895899053628.\n",
            "Loss 1.4590747356414795 and validation accuracy 67.97153024911032: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 12 total loss is 1.5262965857982635 train accuracy is 74.56624605678233.\n",
            "Loss 1.3516429662704468 and validation accuracy 70.1067615658363: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 13 total loss is 1.4417976081371306 train accuracy is 75.74921135646687.\n",
            "Loss 1.253567099571228 and validation accuracy 71.88612099644128: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 14 total loss is 1.3658780209720134 train accuracy is 76.73501577287067.\n",
            "Loss 1.1642158031463623 and validation accuracy 74.02135231316726: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 15 total loss is 1.2971584051847458 train accuracy is 78.07570977917982.\n",
            "Loss 1.082745909690857 and validation accuracy 75.80071174377224: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 16 total loss is 1.234678114950657 train accuracy is 78.90378548895899.\n",
            "Loss 1.0069574117660522 and validation accuracy 76.86832740213524: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 17 total loss is 1.177548672258854 train accuracy is 79.61356466876971.\n",
            "Loss 0.9377040863037109 and validation accuracy 77.22419928825623: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 18 total loss is 1.1251083955168724 train accuracy is 80.48107255520505.\n",
            "Loss 0.8742709159851074 and validation accuracy 77.58007117437722: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 19 total loss is 1.0767559751868248 train accuracy is 81.26971608832808.\n",
            "Loss 0.8152901530265808 and validation accuracy 78.29181494661921: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 20 total loss is 1.0319574914872647 train accuracy is 82.17665615141956.\n",
            "Loss 0.7609823942184448 and validation accuracy 78.64768683274022: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 21 total loss is 0.9903139814734458 train accuracy is 83.28075709779179.\n",
            "Loss 0.7110233902931213 and validation accuracy 79.00355871886121: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 22 total loss is 0.9514986142516136 train accuracy is 83.83280757097792.\n",
            "Loss 0.6648566722869873 and validation accuracy 79.00355871886121: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 23 total loss is 0.9152547419071198 train accuracy is 84.38485804416403.\n",
            "Loss 0.6223288178443909 and validation accuracy 79.35943060498221: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 24 total loss is 0.8812867611646652 train accuracy is 84.97634069400631.\n",
            "Loss 0.5831992626190186 and validation accuracy 79.7153024911032: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 25 total loss is 0.84937689229846 train accuracy is 85.48895899053628.\n",
            "Loss 0.5467873215675354 and validation accuracy 79.7153024911032: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 26 total loss is 0.8193191222846508 train accuracy is 85.8832807570978.\n",
            "Loss 0.5133090019226074 and validation accuracy 79.7153024911032: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 27 total loss is 0.790930800512433 train accuracy is 86.63249211356467.\n",
            "Loss 0.48226723074913025 and validation accuracy 81.13879003558719: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 28 total loss is 0.7641025342047214 train accuracy is 87.14511041009463.\n",
            "Loss 0.45387744903564453 and validation accuracy 81.49466192170819: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 29 total loss is 0.738644079118967 train accuracy is 87.61829652996846.\n",
            "Loss 0.4276662766933441 and validation accuracy 81.85053380782918: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 30 total loss is 0.7144690368324518 train accuracy is 87.89432176656152.\n",
            "Loss 0.4032101333141327 and validation accuracy 82.20640569395017: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 31 total loss is 0.6914167679846287 train accuracy is 88.44637223974763.\n",
            "Loss 0.3805563151836395 and validation accuracy 82.20640569395017: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 32 total loss is 0.6694328583776951 train accuracy is 88.8801261829653.\n",
            "Loss 0.3596189320087433 and validation accuracy 82.56227758007117: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 33 total loss is 0.6484462089836598 train accuracy is 89.27444794952682.\n",
            "Loss 0.34025511145591736 and validation accuracy 83.27402135231317: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 34 total loss is 0.628374483436346 train accuracy is 89.70820189274448.\n",
            "Loss 0.32199180126190186 and validation accuracy 83.27402135231317: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 35 total loss is 0.6091492384672165 train accuracy is 90.02365930599369.\n",
            "Loss 0.3047834038734436 and validation accuracy 83.27402135231317: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 36 total loss is 0.5907028160989285 train accuracy is 90.22082018927445.\n",
            "Loss 0.2887183427810669 and validation accuracy 83.27402135231317: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 37 total loss is 0.5729628123342991 train accuracy is 90.53627760252365.\n",
            "Loss 0.2739814817905426 and validation accuracy 83.27402135231317: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 38 total loss is 0.5558958545327186 train accuracy is 91.00946372239747.\n",
            "Loss 0.2604006826877594 and validation accuracy 83.62989323843416: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 39 total loss is 0.5394646864384413 train accuracy is 91.44321766561514.\n",
            "Loss 0.24774158000946045 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 40 total loss is 0.5236420584842563 train accuracy is 91.71924290220821.\n",
            "Loss 0.23569409549236298 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 41 total loss is 0.5083951549604535 train accuracy is 91.95583596214512.\n",
            "Loss 0.22454021871089935 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 42 total loss is 0.49365274868905545 train accuracy is 92.15299684542586.\n",
            "Loss 0.214127779006958 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 43 total loss is 0.47944923732429745 train accuracy is 92.46845425867508.\n",
            "Loss 0.20451109111309052 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 44 total loss is 0.4657173315063119 train accuracy is 92.90220820189275.\n",
            "Loss 0.19551946222782135 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 45 total loss is 0.45239915121346713 train accuracy is 93.1782334384858.\n",
            "Loss 0.18699467182159424 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 46 total loss is 0.43952555023133755 train accuracy is 93.53312302839117.\n",
            "Loss 0.1788347363471985 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 47 total loss is 0.42705320920795203 train accuracy is 93.84858044164038.\n",
            "Loss 0.17124585807323456 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 48 total loss is 0.41496568638831377 train accuracy is 94.04574132492114.\n",
            "Loss 0.1641078144311905 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 49 total loss is 0.40324947573244574 train accuracy is 94.3217665615142.\n",
            "Loss 0.15729068219661713 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 50 total loss is 0.3918694455176592 train accuracy is 94.5583596214511.\n",
            "Loss 0.1508900374174118 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 51 total loss is 0.38084706515073774 train accuracy is 95.07097791798107.\n",
            "Loss 0.14493529498577118 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 52 total loss is 0.3701511150225997 train accuracy is 95.22870662460568.\n",
            "Loss 0.13928592205047607 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 53 total loss is 0.3597908506169915 train accuracy is 95.78075709779179.\n",
            "Loss 0.13403409719467163 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 54 total loss is 0.34973375760018827 train accuracy is 96.0173501577287.\n",
            "Loss 0.12910205125808716 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 55 total loss is 0.339962656237185 train accuracy is 96.25394321766562.\n",
            "Loss 0.12442971765995026 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 56 total loss is 0.33049416085705163 train accuracy is 96.33280757097792.\n",
            "Loss 0.1200103685259819 and validation accuracy 83.98576512455516: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 57 total loss is 0.321295809186995 train accuracy is 96.52996845425868.\n",
            "Loss 0.11577831208705902 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 58 total loss is 0.3123785418458283 train accuracy is 96.72712933753944.\n",
            "Loss 0.1117873266339302 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 59 total loss is 0.3037189385853708 train accuracy is 96.80599369085174.\n",
            "Loss 0.10793507844209671 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 60 total loss is 0.29531932482495904 train accuracy is 96.96372239747635.\n",
            "Loss 0.10430353134870529 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 61 total loss is 0.2871758952736855 train accuracy is 97.12145110410094.\n",
            "Loss 0.10086078941822052 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 62 total loss is 0.27927449345588684 train accuracy is 97.20031545741325.\n",
            "Loss 0.09751981496810913 and validation accuracy 84.34163701067615: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 63 total loss is 0.2716089554131031 train accuracy is 97.3186119873817.\n",
            "Loss 0.09444889426231384 and validation accuracy 84.69750889679716: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 64 total loss is 0.2641715587116778 train accuracy is 97.43690851735016.\n",
            "Loss 0.09149286895990372 and validation accuracy 84.69750889679716: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 65 total loss is 0.2569492504931986 train accuracy is 97.47634069400631.\n",
            "Loss 0.08864926546812057 and validation accuracy 84.69750889679716: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 66 total loss is 0.24994417373090982 train accuracy is 97.59463722397476.\n",
            "Loss 0.0859602764248848 and validation accuracy 84.69750889679716: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 67 total loss is 0.2431516692042351 train accuracy is 97.75236593059937.\n",
            "Loss 0.08339027315378189 and validation accuracy 84.69750889679716: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 68 total loss is 0.23656866252422332 train accuracy is 97.91009463722398.\n",
            "Loss 0.08095862716436386 and validation accuracy 85.05338078291815: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 69 total loss is 0.2301906925626099 train accuracy is 97.94952681388013.\n",
            "Loss 0.07863099873065948 and validation accuracy 85.05338078291815: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 70 total loss is 0.22400212436914443 train accuracy is 98.06782334384857.\n",
            "Loss 0.07636833190917969 and validation accuracy 85.05338078291815: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 71 total loss is 0.21800733776763082 train accuracy is 98.18611987381703.\n",
            "Loss 0.07419349253177643 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 72 total loss is 0.2121856028214097 train accuracy is 98.3044164037855.\n",
            "Loss 0.07207707315683365 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 73 total loss is 0.2065492456778884 train accuracy is 98.34384858044164.\n",
            "Loss 0.07006967812776566 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 74 total loss is 0.20108865508809687 train accuracy is 98.3832807570978.\n",
            "Loss 0.06817449629306793 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 75 total loss is 0.1958042150363326 train accuracy is 98.46214511041009.\n",
            "Loss 0.0663653165102005 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 76 total loss is 0.19067386742681264 train accuracy is 98.5410094637224.\n",
            "Loss 0.06463083624839783 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 77 total loss is 0.18569972310215235 train accuracy is 98.58044164037855.\n",
            "Loss 0.06296620517969131 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 78 total loss is 0.18086861339397728 train accuracy is 98.65930599369085.\n",
            "Loss 0.061385538429021835 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 79 total loss is 0.17618916807696222 train accuracy is 98.698738170347.\n",
            "Loss 0.059872761368751526 and validation accuracy 85.76512455516014: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 80 total loss is 0.17165579637512565 train accuracy is 98.77760252365931.\n",
            "Loss 0.05841336399316788 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 81 total loss is 0.1672648822888732 train accuracy is 98.93533123028391.\n",
            "Loss 0.05698540806770325 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 82 total loss is 0.16300971265882253 train accuracy is 99.01419558359622.\n",
            "Loss 0.0556381419301033 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 83 total loss is 0.1589017955120653 train accuracy is 99.01419558359622.\n",
            "Loss 0.05434279516339302 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 84 total loss is 0.15491177844814957 train accuracy is 99.09305993690852.\n",
            "Loss 0.053079862147569656 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 85 total loss is 0.15105398846790194 train accuracy is 99.13249211356467.\n",
            "Loss 0.051880672574043274 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 86 total loss is 0.14731585304252803 train accuracy is 99.17192429022082.\n",
            "Loss 0.050709303468465805 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 87 total loss is 0.14369659996591508 train accuracy is 99.25078864353313.\n",
            "Loss 0.04958915337920189 and validation accuracy 86.47686832740213: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 88 total loss is 0.140187695203349 train accuracy is 99.32965299684543.\n",
            "Loss 0.04845919460058212 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 89 total loss is 0.13679553312249482 train accuracy is 99.32965299684543.\n",
            "Loss 0.04738565906882286 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 90 total loss is 0.13349784719757735 train accuracy is 99.32965299684543.\n",
            "Loss 0.04634232819080353 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 91 total loss is 0.13030974245630206 train accuracy is 99.36908517350157.\n",
            "Loss 0.0453738272190094 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 92 total loss is 0.12722044819965958 train accuracy is 99.44794952681389.\n",
            "Loss 0.04441401734948158 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 93 total loss is 0.12423368892632425 train accuracy is 99.48738170347004.\n",
            "Loss 0.04349015653133392 and validation accuracy 86.83274021352314: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 94 total loss is 0.121334732696414 train accuracy is 99.52681388012618.\n",
            "Loss 0.04260018840432167 and validation accuracy 87.18861209964413: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 95 total loss is 0.11852427679114044 train accuracy is 99.52681388012618.\n",
            "Loss 0.041735753417015076 and validation accuracy 87.18861209964413: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 96 total loss is 0.11579567631706596 train accuracy is 99.52681388012618.\n",
            "Loss 0.04088276997208595 and validation accuracy 87.18861209964413: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 97 total loss is 0.1131578438449651 train accuracy is 99.56624605678233.\n",
            "Loss 0.040063630789518356 and validation accuracy 87.18861209964413: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 98 total loss is 0.11059727864339948 train accuracy is 99.56624605678233.\n",
            "Loss 0.03928356617689133 and validation accuracy 87.54448398576513: \n",
            "learning rate updated to :  [0.0001]\n",
            "end of epoch 99 total loss is 0.10812055692076683 train accuracy is 99.64511041009463.\n",
            "Loss 0.03851618617773056 and validation accuracy 87.54448398576513: \n",
            "learning rate updated to :  [0.0001]\n"
          ]
        }
      ],
      "source": [
        "# from tqdm import tqdm\n",
        "from torch.optim import lr_scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam( filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "# optimizer = optim.Adam(  model.parameters(), lr=2e-5)\n",
        "optimizer = optim.SGD( model.parameters(), lr=0.0001, momentum=0.9)\n",
        "# optimizer = optim.SGD( filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001, momentum=0.9)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=0.9, gamma=0.1)\n",
        "\n",
        "\n",
        "# Train the model yourself with this cell\n",
        "log_update_frequency = 1\n",
        "\n",
        "all_loss = []\n",
        "model.train()\n",
        "epochs = 100\n",
        "for epoch in range(1,epochs):\n",
        "  preds, true  = [], []\n",
        "  correct = 0\n",
        "  t=0\n",
        "  for batch_images,batch_labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(\n",
        "        batch_images.to(device)\n",
        "    )\n",
        "\n",
        "    loss = criterion(logits, batch_labels.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    all_loss.append(loss_value)\n",
        "\n",
        "    probs = softmax(logits,1)\n",
        "    preds = torch.argmax(probs,1)#.tolist()\n",
        "    correct+= torch.sum( preds == batch_labels.to(device) ).item() \n",
        "    t+=batch_labels.size(0)\n",
        "\n",
        "  train_accuracy = 100 * correct/t\n",
        "  print('end of epoch {} total loss is {} train accuracy is {}.'.format(epoch,np.array(all_loss).mean(), train_accuracy ) )\n",
        "  all_loss = []\n",
        "  correct=0\n",
        "  t=0\n",
        "  \n",
        "  # if epoch == 3:\n",
        "  #   dfs_unfreeze(model)\n",
        "  #   print('weights are unfrozen!')\n",
        "\n",
        "  if epoch % log_update_frequency == 0:\n",
        "    print('Loss {} and validation accuracy {}: '.format(loss_value, evaluate(val_loader) ) )\n",
        "    # scheduler.step()\n",
        "    print('learning rate updated to : ',scheduler.get_last_lr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "87.54448398576513"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "office_loader.setup('test')\n",
        "test_loader = office_loader.test_dataloader()\n",
        "evaluate(val_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "image_classification_resnet.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('test')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5290e27e2982e14914fc743eb271efd553283179009389fbf33907a815e7eb33"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
